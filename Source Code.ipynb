{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a808e47",
   "metadata": {},
   "source": [
    "# Sample Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f06cb3",
   "metadata": {},
   "source": [
    "## MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1706d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten,\n",
    "GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "# Define dataset path\n",
    "dataset_path = \"/content/drive/MyDrive/Vehicles\"\n",
    "# Preprocessing using ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    " rescale=1.0/255.0,\n",
    " validation_split=0.2, # 20% validation split\n",
    ")\n",
    "# Training and validation data generators\n",
    "train_generator = datagen.flow_from_directory(\n",
    " dataset_path,\n",
    " target_size=(224, 224),\n",
    " batch_size=32,\n",
    " class_mode='categorical',\n",
    " subset='training',\n",
    ")\n",
    "validation_generator = datagen.flow_from_directory(\n",
    " dataset_path,\n",
    " target_size=(224, 224),\n",
    " batch_size=32,\n",
    " class_mode='categorical',\n",
    " subset='validation',\n",
    ")\n",
    "# Get class indices\n",
    "class_labels = train_generator.class_indices\n",
    "print(\"Class Labels:\", class_labels)\n",
    "# Load pre-trained MobileNetV2 model (exclude the top layer)\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False,\n",
    "weights='imagenet')\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "# Add custom classification head\n",
    "model = Sequential([\n",
    " base_model,\n",
    " GlobalAveragePooling2D(),\n",
    " Dense(128, activation='relu'),\n",
    " Dropout(0.5),\n",
    " Dense(len(class_labels), activation='softmax') # Output layer with the number of\n",
    "classes\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Train the model\n",
    "history = model.fit(\n",
    " train_generator,\n",
    " validation_data=validation_generator,\n",
    " epochs=10,\n",
    " steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    " validation_steps=validation_generator.samples // validation_generator.batch_size\n",
    ")\n",
    "# Evaluate the model\n",
    "validation_generator.reset()\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Validation Loss: {loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
    "# Classification report\n",
    "validation_generator.reset()\n",
    "predictions = model.predict(validation_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = validation_generator.classes\n",
    "class_labels = list(validation_generator.class_indices.keys())\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_classes, predicted_classes,\n",
    "target_names=class_labels))\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "# Unfreeze some layers in the base model\n",
    "base_model.trainable = True\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Fine-tune the model\n",
    "history_fine_tune = model.fit(\n",
    " train_generator,\n",
    " validation_data=validation_generator,\n",
    " epochs=5,\n",
    " steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    " validation_steps=validation_generator.samples // validation_generator.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066fed6d",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Function to split dataset into training and validation sets\n",
    "def split_dataset(dataset_dir, output_dir, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Splits a dataset into train and validation sets.\n",
    "    Args:\n",
    "        dataset_dir (str): Path to the original dataset directory.\n",
    "        output_dir (str): Path to the output directory for train and validation splits.\n",
    "        train_ratio (float): Ratio of training data. Default is 0.8 (80% training, 20% validation).\n",
    "    \"\"\"\n",
    "    train_dir = os.path.join(output_dir, 'train')\n",
    "    val_dir = os.path.join(output_dir, 'val')\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over each class folder\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        # Create class subdirectories in train and val folders\n",
    "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
    "\n",
    "        # Get all file names in the class directory\n",
    "        files = os.listdir(class_dir)\n",
    "        random.shuffle(files)\n",
    "\n",
    "        # Split files into train and validation sets\n",
    "        split_idx = int(len(files) * train_ratio)\n",
    "        train_files = files[:split_idx]\n",
    "        val_files = files[split_idx:]\n",
    "\n",
    "        # Move files to respective directories\n",
    "        for file_name in train_files:\n",
    "            src = os.path.join(class_dir, file_name)\n",
    "            dst = os.path.join(train_dir, class_name, file_name)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "        for file_name in val_files:\n",
    "            src = os.path.join(class_dir, file_name)\n",
    "            dst = os.path.join(val_dir, class_name, file_name)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "    print(f\"Dataset split completed. Train and val sets are saved in {output_dir}\")\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Split the dataset (adjust paths as needed)\n",
    "original_dataset_path = \"/content/drive/MyDrive/Vehicle Classification/Vehicles\"  # Replace with your original dataset path\n",
    "split_output_path = \"/content/drive/MyDrive/vgg19/split output \"         # Replace with the path to save the split dataset\n",
    "split_dataset(original_dataset_path, split_output_path)\n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for VGG-19\n",
    "])\n",
    "\n",
    "# Load train and validation datasets\n",
    "train_dataset = ImageFolder(root=os.path.join(split_output_path, \"train\"), transform=transform)\n",
    "val_dataset = ImageFolder(root=os.path.join(split_output_path, \"val\"), transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the VGG-19 model\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "\n",
    "# Modify the final layer to match the number of vehicle classes\n",
    "num_classes = len(train_dataset.classes)  # Number of vehicle categories\n",
    "vgg19.classifier[6] = nn.Linear(vgg19.classifier[6].in_features, num_classes)\n",
    "\n",
    "# Move the model to the device\n",
    "vgg19 = vgg19.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vgg19.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    vgg19.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = vgg19(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "vgg19.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vgg19(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Save the model\n",
    "torch.save(vgg19.state_dict(), 'vgg19_vehicle_classification.pth')\n",
    "\n",
    "# Class Names\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to generate classification report\n",
    "def generate_classification_report(model, data_loader, class_names, device):\n",
    "    \"\"\"\n",
    "    Generates a classification report for the given model and data loader.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model.\n",
    "        data_loader: DataLoader for validation or test dataset.\n",
    "        class_names: List of class names.\n",
    "        device: Device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        Classification report as a string.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    return report\n",
    "\n",
    "# Generate and print classification report\n",
    "classification_report_str = generate_classification_report(vgg19, val_loader, train_dataset.classes, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95f4a1",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d7640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import models\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix  # Import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Function to split dataset into train and validation sets\n",
    "def split_dataset(dataset_dir, output_dir, train_ratio=0.8):\n",
    "    train_dir = os.path.join(output_dir, 'train')\n",
    "    val_dir = os.path.join(output_dir, 'val')\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
    "\n",
    "        files = os.listdir(class_dir)\n",
    "        random.shuffle(files)\n",
    "\n",
    "        split_idx = int(len(files) * train_ratio)\n",
    "        train_files = files[:split_idx]\n",
    "        val_files = files[split_idx:]\n",
    "\n",
    "        for file_name in train_files:\n",
    "            shutil.copy2(os.path.join(class_dir, file_name), os.path.join(train_dir, class_name, file_name))\n",
    "\n",
    "        for file_name in val_files:\n",
    "            shutil.copy2(os.path.join(class_dir, file_name), os.path.join(val_dir, class_name, file_name))\n",
    "\n",
    "    print(f\"Dataset split completed. Train and val sets are saved in {output_dir}\")\n",
    "\n",
    "# Dataset transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split the dataset (replace with actual dataset paths)\n",
    "split_dataset('/content/drive/MyDrive/Vehicle Classification/Vehicles', '/content/drive/MyDrive/ResNet50/spilt data')\n",
    "\n",
    "# Load datasets\n",
    "train_dir = '/content/drive/MyDrive/ResNet50/spilt data/train'\n",
    "val_dir = '/content/drive/MyDrive/ResNet50/spilt data/val'\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
    "val_dataset = torchvision.datasets.ImageFolder(val_dir, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load ResNet-50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the final layer for the number of classes in your dataset\n",
    "num_classes = len(train_dataset.classes)\n",
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n",
    "\n",
    "# Move model to device\n",
    "resnet50 = resnet50.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet50.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    resnet50.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = resnet50(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(resnet50.state_dict(), 'resnet50_vehicle_classification.pth')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, class_names):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Generate confusion matrix\n",
    "def plot_confusion_matrix(all_labels, all_preds, class_names):\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45, ax=plt.gca())\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Generate F1 score\n",
    "def compute_f1_score(all_labels, all_preds):\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "# Generate classification report\n",
    "def generate_classification_report(all_labels, all_preds, class_names):\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "# Evaluate and generate metrics\n",
    "all_labels, all_preds = evaluate_model(resnet50, val_loader, train_dataset.classes)\n",
    "plot_confusion_matrix(all_labels, all_preds, train_dataset.classes)\n",
    "compute_f1_score(all_labels, all_preds)\n",
    "generate_classification_report(all_labels, all_preds, train_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a4c9f",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet requires 224x224 input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the dataset (replace with your dataset path)\n",
    "dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Vehicle Classification/Vehicles', transform=transform)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pre-trained ResNet model\n",
    "resnet = models.resnet18(pretrained=True)  # Use ResNet-18, you can choose ResNet-50, etc.\n",
    "\n",
    "# Modify the final layer for 4-class classification\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_features, 4)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total\n",
    "\n",
    "# Validation loop\n",
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total\n",
    "\n",
    "# Training the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(resnet, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = validate(resnet, val_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(resnet.state_dict(), 'vehicle_classification_resnet.pth')\n",
    "\n",
    "from sklearn.metrics import classification_report  # Import classification_report\n",
    "\n",
    "# ... (Your existing training code) ...\n",
    "\n",
    "# Get predictions for the entire dataset (train and validation)\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Assuming you want to get the predictions for your validation set\n",
    "# You might want to iterate through the entire dataset or a specific split like val_loader\n",
    "for inputs, labels in val_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = resnet(inputs)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    all_labels.extend(labels.cpu().numpy())  # Move labels to CPU and convert to numpy\n",
    "    all_predictions.extend(predicted.cpu().numpy())  # Move predictions to CPU and convert to numpy\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(all_labels, all_predictions, target_names=val_loader.dataset.dataset.classes)  # Use correct loader\n",
    "print(report) # Corrected indentation\n",
    "\n",
    "!pip install scikit-learn  # Install scikit-learn if you haven't already\n",
    "\n",
    "# Get predictions for the entire validation set\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "for inputs, labels in val_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = resnet(inputs)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "    all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(all_labels, all_predictions, target_names=val_loader.dataset.dataset.classes)\n",
    "print(report)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=val_loader.dataset.dataset.classes,\n",
    "            yticklabels=val_loader.dataset.dataset.classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
